# -*- coding: utf-8 -*-
"""Speech to  Text 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fdhlJNZp-9rd1jUxlDm6dJVYebIaNh7o
"""

import numpy as np
import pandas as pd
file_path = '/content/drive/MyDrive/Dataset/cv-valid-train.csv'

dftrain = pd.read_csv(file_path)
dftrain.info()

pd.set_option('display.max_colwidth', None)
dftrain.head()

!pip install pydub

from IPython.display import display, Audio
import os
from pydub import AudioSegment

# Function to play audio file
def play_audio(audio_path):
    display(Audio(filename=audio_path))

# Specify the folder path containing MP3 files
folder_path = '/content/drive/MyDrive/Dataset'

# Get the list of all files in the folder and sort them
all_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.mp3')])

# Get the total file count
total_files = len(all_files)
print(f'Total number of MP3 files in the folder: {total_files}')

# Play the first two audio files
for i in range(min(10, len(all_files))):
    file_path = os.path.join(folder_path, all_files[i])
    audio = AudioSegment.from_mp3(file_path)

    print(f'Playing file: {all_files[i]}')
    play_audio(file_path)

!pip install SpeechRecognition

import os
import speech_recognition as sr
from pydub import AudioSegment

# Set the path to the audio files
audio_folder = '/content/drive/MyDrive/Dataset'

# Get a list of all audio files in the folder
audio_files = [file for file in os.listdir(audio_folder) if file.endswith('.mp3')]

# Sort the audio files
audio_files.sort()

# Take the first 10 audio files
selected_audio_files = audio_files[:10]

# Initialize the recognizer
recognizer = sr.Recognizer()

# Process each selected audio file
for audio_file in selected_audio_files:
    # Load the audio file
    audio_path = os.path.join(audio_folder, audio_file)
    audio = AudioSegment.from_mp3(audio_path)

    # Convert the audio to WAV format (required by SpeechRecognition)
    audio.export("temp.wav", format="wav")

    # Recognize speech using Google Web Speech API
    with sr.AudioFile("temp.wav") as source:
        audio_data = recognizer.record(source)
        try:
            # Perform speech recognition
            text = recognizer.recognize_google(audio_data)
            print(f"Audio File: {audio_file}\nTranscription: {text}\n")
        except sr.UnknownValueError:
            print(f"Audio File: {audio_file}\nTranscription not recognized\n")
        except sr.RequestError as e:
            print(f"Audio File: {audio_file}\nError with the speech recognition service; {e}\n")

# Clean up the temporary WAV file
os.remove("temp.wav")

import os
from pydub import AudioSegment, silence
from concurrent.futures import ThreadPoolExecutor
from pydub.playback import play

# Function to adjust volume and remove background noise
def process_audio(file_path, reference_audio):
    # Load original audio
    original_audio = AudioSegment.from_mp3(file_path)

    # Adjust volume
    adjusted_audio = original_audio.apply_gain(reference_audio.dBFS - original_audio.dBFS)

    # Remove background noise
    adjusted_audio = remove_background_noise(adjusted_audio)

    # Trim or pad to match the length of the original audio with an extra 2 seconds
    adjusted_audio = match_audio_length(original_audio, adjusted_audio, extra_padding=2000)

    return adjusted_audio

# Function to remove background noise
def remove_background_noise(audio, silence_threshold=-30, keep_silence=100):
    # Split audio on silence and filter out silent chunks
    chunks = silence.split_on_silence(audio, silence_thresh=silence_threshold, keep_silence=keep_silence)
    filtered_audio = AudioSegment.silent()
    for chunk in chunks:
        filtered_audio += chunk

    return filtered_audio

# Function to match the length of two audio segments
def match_audio_length(original_audio, adjusted_audio, extra_padding=0):
    # Calculate the target length with extra padding
    target_length = len(original_audio) + extra_padding

    # Trim or pad to match the target length
    if len(adjusted_audio) < target_length:
        adjusted_audio = adjusted_audio + AudioSegment.silent(duration=target_length - len(adjusted_audio))
    elif len(adjusted_audio) > target_length:
        adjusted_audio = adjusted_audio[:target_length]

    return adjusted_audio

# Specify the folder path containing MP3 files
folder_path = '/content/drive/MyDrive/Dataset'

# Specify the reference file for volume adjustment
reference_file = 'sample-000000.mp3'
reference_file_path = os.path.join(folder_path, reference_file)
reference_audio = AudioSegment.from_mp3(reference_file_path)

# Process only the first 10 audio files
audio_files = [file for file in os.listdir(folder_path) if file.endswith('.mp3')]
audio_files.sort()
selected_audio_files = audio_files[:10]

# Counter for tracking the number of files being processed
files_processed_counter = 0

# Process audio files (adjust volume and remove background noise) using parallelization
with ThreadPoolExecutor() as executor:
    # List of futures representing the ongoing audio processing tasks
    futures_process = []

    for file_name in selected_audio_files:
        file_path = os.path.join(folder_path, file_name)

        # Process audio (adjust volume and remove background noise)
        futures_process.append(executor.submit(process_audio, file_path, reference_audio))
        files_processed_counter += 1
        print(f'\rProcessing audio: {files_processed_counter} files processed', end='', flush=True)

    # Wait for all audio processing tasks to complete and retrieve results
    processed_audios = [future.result() for future in futures_process]

# Print the names of the processed audio files
print("\nNames of processed audio files:")
for i, file_name in enumerate(selected_audio_files):
    print(f"{i + 1}. {file_name}")

from IPython.display import Audio, display

# Specify the folder path to save the processed audio files
output_folder = '/content/drive/MyDrive/Dataset/Output'

# Save the processed audio files with the same names
for i, audio in enumerate(processed_audios):
    file_name, file_extension = os.path.splitext(selected_audio_files[i])
    output_file_path = os.path.join(output_folder, f'{file_name}.mp3')
    audio.export(output_file_path, format='mp3')

    print(f'Saved processed audio: {output_file_path}')

    # Play the processed audio
    print(f'Playing Adjusted Audio: {selected_audio_files[i]}')
    display(Audio(output_file_path))

!pip install python-Levenshtein

import os
import pandas as pd
import speech_recognition as sr
import Levenshtein as lev
from pydub import AudioSegment
from pydub.effects import normalize

# Set the path to the audio files
csv_path = '/content/drive/MyDrive/Dataset/cv-valid-train.csv'

# Read the CSV file
df = pd.read_csv(csv_path)

# Use the processed audio files generated in the previous code
#processed_audios = [AudioSegment.from_mp3(file) for file in os.listdir('/kaggle/working') if file.startswith('processed_audio')]

# Take the first 10 processed audio files
selected_processed_audios = processed_audios[:10]

# Initialize the recognizer
recognizer = sr.Recognizer()

for i, audio in enumerate(selected_processed_audios):
    # Apply volume adjustment
    audio = normalize(audio)

    # Export the processed audio to a temporary WAV file
    audio.export("temp.wav", format="wav")

    # Retrieve actual text from the CSV based on filename
    filename_from_text = f'sample-{i:06d}.mp3'
    actual_text = df[df['filename'].str.split('/').str[-1] == filename_from_text]['text'].values


    if len(actual_text) > 0:
        actual_text = actual_text[0]

        # Recognize speech using Google Web Speech API
        with sr.AudioFile("temp.wav") as source:
            audio_data = recognizer.record(source)
            try:
                # Perform speech recognition
                text = recognizer.recognize_google(audio_data)
                # Calculate WER score
                wer_score = lev.distance(text.split(), actual_text.split()) / len(actual_text.split())

                # Print results
                print(f"Filename: {filename_from_text}")
                print(f"Predicted Text: {text}")
                print(f"Actual Text: {actual_text}")
                print(f"WER Score: {wer_score}\n")
            except sr.UnknownValueError:
                print(f"Audio File: {filename_from_text}\nTranscription not recognized\n")
            except sr.RequestError as e:
                print(f"Audio File: {filename_from_text}\nError with the speech recognition service; {e}\n")
    else:
        print(f"Actual text not found in CSV file for {filename_from_text}\n")

# Clean up the temporary WAV file
os.remove("temp.wav")

!pip install translate

!pip install googletrans==4.0.0-rc1

!pip install gtts

from IPython.display import Audio, display
import os
from translate import Translator
from gtts import gTTS

# Iterate over the results
for i, audio in enumerate(selected_processed_audios):
    # Apply volume adjustment
    audio = normalize(audio)

    # Export the processed audio to a temporary WAV file
    audio.export("temp.wav", format="wav")

    # Retrieve predicted text
    with sr.AudioFile("temp.wav") as source:
        audio_data = recognizer.record(source)
        try:
            # Perform speech recognition
            predicted_text = recognizer.recognize_google(audio_data)

            # Translate predicted text to Turkish
            translator = Translator(to_lang="ta")
            translated_text = translator.translate(predicted_text)

            # Print results
            print(f"Filename: {filename_from_text}")
            print(f"Predicted Text (English): {predicted_text}")
            print(f"Predicted Text (Tamil): {translated_text}")

            # Save translated audio
            tts = gTTS(translated_text, lang='tr')
            translated_audio_path = f"translated_audio_{i}.mp3"
            tts.save(translated_audio_path)

            # Play the translated audio
            display(Audio(translated_audio_path))

        except sr.UnknownValueError:
            print(f"Audio File: {filename_from_text}\nTranscription not recognized\n")
        except sr.RequestError as e:
            print(f"Audio File: {filename_from_text}\nError with the speech recognition service; {e}\n")

# Clean up the temporary WAV file
os.remove("temp.wav")

# Specify the file path
file_path = '/content/drive/MyDrive/Dataset/cv-valid-train.csv'

# Read the CSV file into a DataFrame
dftrain = pd.read_csv(file_path)
dftrain.info()

# Drop rows where 'gender' column is null
dftrain = dftrain.dropna(subset=['gender'])

dftrain.info()

dftrain.head()

import os
import librosa
import librosa.display
import matplotlib.pyplot as plt

audio_folder_path = '/content/drive/MyDrive/Dataset'

# Extract two female and two male filenames from dftrain
female_files = dftrain[dftrain['gender'] == 'female']['filename'].head(2)
male_files = dftrain[dftrain['gender'] == 'male']['filename'].head(2)

# Function to plot waveform, spectrogram, and MFCC
def plot_features(file_path, gender):
    audio, sr = librosa.load(file_path, sr=None)

    plt.figure(figsize=(15, 8))

    # Plot Waveform
    plt.subplot(3, 1, 1)
    librosa.display.waveshow(audio, sr=sr)
    plt.title(f'Waveform - {gender}')

    # Plot Spectrogram
    plt.subplot(3, 1, 2)
    S = librosa.feature.melspectrogram(y=audio, sr=sr)
    librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', x_axis='time')
    plt.title(f'Spectrogram - {gender}')

    # Plot MFCC
    plt.subplot(3, 1, 3)
    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
    librosa.display.specshow(mfccs, x_axis='time')
    plt.title(f'MFCC - {gender}')

    plt.tight_layout()
    plt.show()

# Plot features for female files
for file in female_files:
    file_path = os.path.join(audio_folder_path, file.split('/')[-1])
    print(f"Plotting features for {file_path}")
    plot_features(file_path, 'Female')

# Plot features for male files
for file in male_files:
    file_path = os.path.join(audio_folder_path, file.split('/')[-1])
    print(f"Plotting features for {file_path}")
    plot_features(file_path, 'Male')

!pip install pyworld

import os
import librosa
import pyworld as pw
import matplotlib.pyplot as plt

# actual path to your audio files
audio_folder_path = '/content/drive/MyDrive/Dataset'

# Extract two female and two male filenames from dftrain
female_files = dftrain[dftrain['gender'] == 'female']['filename'].head(2)
male_files = dftrain[dftrain['gender'] == 'male']['filename'].head(2)

def plot_pitch_contour(file_path, gender, subplot_index):
    # Load audio file
    audio, sr = librosa.load(file_path, sr=None)

    # Convert audio data to double
    audio = audio.astype('double')

    # Extract pitch using pyworld
    _f0, t = pw.dio(audio, sr)  # Dio algorithm
    f0 = pw.stonemask(audio, _f0, t, sr)

    # Plot pitch contour
    plt.subplot(2, 2, subplot_index)
    plt.plot(t, f0)
    plt.title(f'Pitch Contour - {gender}')
    plt.xlabel('Time(s)')
    plt.ylabel('Pitch (Hz)')

# Plot pitch contour for female files
plt.figure(figsize=(12, 8))
for i, file in enumerate(female_files, 1):
    file_path = os.path.join(audio_folder_path, file.split('/')[-1])
    print(f"Plotting pitch contour for {file_path}")
    plot_pitch_contour(file_path, 'Female', i)

# Plot pitch contour for male files
for i, file in enumerate(male_files, 1):
    file_path = os.path.join(audio_folder_path, file.split('/')[-1])
    print(f"Plotting pitch contour for {file_path}")
    plot_pitch_contour(file_path, 'Male', i + 2)
plt.tight_layout()
plt.show()